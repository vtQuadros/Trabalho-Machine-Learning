{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac51e739",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö PARTE 1: PREPARA√á√ÉO E LIMPEZA DE DADOS\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotecas importadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa72bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "df = pd.read_csv(\"docs/treino.csv\")\n",
    "\n",
    "print(f\"Dados carregados: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fd2a4",
   "metadata": {},
   "source": [
    "## 1.2 An√°lise Inicial e Tratamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ab8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegar informa√ß√µes da Tabela\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover duplicatas\n",
    "linhas_antes = len(df)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "linhas_depois = len(df)\n",
    "\n",
    "print(f\"Linhas antes: {linhas_antes}\")\n",
    "print(f\"Linhas depois: {linhas_depois}\")\n",
    "print(f\"Duplicatas removidas: {linhas_antes - linhas_depois}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter latitude e longitude para float\n",
    "df['latitude'] = df['latitude'].astype(str).str.replace(',', '.').astype(float)\n",
    "df['longitude'] = df['longitude'].astype(str).str.replace(',', '.').astype(float)\n",
    "\n",
    "# Converter data\n",
    "df['dt_ocorrencia'] = pd.to_datetime(df['dt_ocorrencia'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "print(\"Convers√µes realizadas!\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff948518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preencher valores ausentes - num√©ricos com mediana\n",
    "df['peso_max_decolagem'].fillna(df['peso_max_decolagem'].median(), inplace=True)\n",
    "df['numero_assentos'].fillna(df['numero_assentos'].median(), inplace=True)\n",
    "\n",
    "# Preencher valores ausentes - categ√≥ricos com moda\n",
    "df['op_padronizado'].fillna(df['op_padronizado'].mode()[0], inplace=True)\n",
    "df['hr_ocorrencia'].fillna(df['hr_ocorrencia'].mode()[0], inplace=True)\n",
    "df['regiao'].fillna(df['regiao'].mode()[0], inplace=True)\n",
    "df['fase_operacao'].fillna(df['fase_operacao'].mode()[0], inplace=True)\n",
    "df['modelo_aeronave'].fillna(df['modelo_aeronave'].mode()[0], inplace=True)\n",
    "df['nome_fabricante'].fillna(df['nome_fabricante'].mode()[0], inplace=True)\n",
    "\n",
    "# Remover linhas com dados essenciais ausentes\n",
    "df.dropna(subset=['dt_ocorrencia', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "print(\"Tratamento de valores ausentes conclu√≠do!\")\n",
    "print(f\"Total de linhas ap√≥s tratamento: {len(df)}\")\n",
    "print(\"\\nValores nulos restantes:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar novas colunas de ano e m√™s\n",
    "df['ano_ocorrencia'] = df['dt_ocorrencia'].dt.year\n",
    "df['mes_ocorrencia'] = df['dt_ocorrencia'].dt.month\n",
    "\n",
    "print(\"Novas colunas criadas!\")\n",
    "df[['dt_ocorrencia', 'ano_ocorrencia', 'mes_ocorrencia']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento dos valores (NaN)\n",
    "\n",
    "print(\"\\n--- Contagem de valores nulos ANTES do tratamento ---\")\n",
    "\n",
    "\n",
    "# Lista de colunas num√©ricas para imputar com a mediana\n",
    "colunas_numericas_nan = ['peso_max_decolagem', 'numero_assentos']\n",
    "for col in colunas_numericas_nan:\n",
    "    mediana = df[col].median()\n",
    "    df[col] = df[col].fillna(mediana)\n",
    "    print(f\"Valores nulos em '{col}' preenchidos com a mediana: {mediana}\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Lista de colunas categ√≥ricas para imputar com a moda\n",
    "colunas_categoricas_nan = ['op_padronizado', 'hr_ocorrencia', 'regiao', 'fase_operacao', 'modelo_aeronave', 'nome_fabricante']\n",
    "for col in colunas_categoricas_nan:\n",
    "    moda = df[col].mode()[0]\n",
    "    df[col] = df[col].fillna(moda)\n",
    "    print(f\"Valores nulos em '{col}' preenchidos com a moda: '{moda}'\")\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Removendo linhas onde dados essenciais ainda s√£o nulos\n",
    "print(\"Removendo linhas onde 'dt_ocorrencia', 'latitude' ou 'longitude' s√£o nulos...\")\n",
    "df.dropna(subset=['dt_ocorrencia', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "print(\"\\n--- Contagem de valores nulos DEPOIS do tratamento ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar features e target\n",
    "features = ['latitude', 'longitude', 'peso_max_decolagem', 'numero_assentos',\n",
    "            'fase_operacao', 'cat_aeronave', 'regiao', 'uf', 'modelo_aeronave', \n",
    "            'nome_fabricante', 'ano_ocorrencia', 'mes_ocorrencia']\n",
    "\n",
    "X = df[features]\n",
    "y = df['les_fatais_trip']\n",
    "\n",
    "print(f\"Features selecionadas: {X.shape[1]}\")\n",
    "print(f\"Total de registros: {X.shape[0]}\")\n",
    "\n",
    "# Verificar balanceamento\n",
    "print(\"\\nDistribui√ß√£o da vari√°vel target:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Visualizar balanceamento\n",
    "plt.figure(figsize=(8, 5))\n",
    "y.value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Acidentes Fatais vs N√£o Fatais')\n",
    "plt.xlabel('Classe (0=N√£o Fatal, 1=Fatal)')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af12f2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE 2: PREPARA√á√ÉO PARA MODELAGEM\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Divis√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de treino: {len(X_train)} linhas\")\n",
    "print(f\"Conjunto de teste: {len(X_test)} linhas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bac136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar colunas num√©ricas e categ√≥ricas\n",
    "colunas_numericas = ['latitude', 'longitude', 'peso_max_decolagem', 'numero_assentos', \n",
    "                     'ano_ocorrencia', 'mes_ocorrencia']\n",
    "colunas_categoricas = ['fase_operacao', 'cat_aeronave', 'regiao', 'uf', \n",
    "                       'modelo_aeronave', 'nome_fabricante']\n",
    "\n",
    "print(\"Colunas num√©ricas:\", colunas_numericas)\n",
    "print(\"Colunas categ√≥ricas:\", colunas_categoricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar vari√°veis categ√≥ricas com get_dummies\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=colunas_categoricas)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=colunas_categoricas)\n",
    "\n",
    "# Garantir que treino e teste tenham as mesmas colunas\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "print(f\"Features ap√≥s encoding: {X_train_encoded.shape[1]}\")\n",
    "\n",
    "# Normalizar features num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "print(\"Pr√©-processamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bbc032",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü§ñ PARTE 3: MODELAGEM AVAN√áADA\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Pr√©-processamento Avan√ßado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dee68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE para balancear as classes\n",
    "print(\"Antes do SMOTE:\")\n",
    "print(f\"Classe 0 (N√£o Fatal): {sum(y_train == 0)}\")\n",
    "print(f\"Classe 1 (Fatal): {sum(y_train == 1)}\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nDepois do SMOTE:\")\n",
    "print(f\"Classe 0 (N√£o Fatal): {sum(y_train_balanced == 0)}\")\n",
    "print(f\"Classe 1 (Fatal): {sum(y_train_balanced == 1)}\")\n",
    "\n",
    "# Visualizar balanceamento\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "pd.Series(y_train).value_counts().plot(kind='bar', ax=axes[0], color=['skyblue', 'salmon'])\n",
    "axes[0].set_title('Antes do SMOTE')\n",
    "axes[0].set_xlabel('Classe')\n",
    "axes[0].set_ylabel('Quantidade')\n",
    "\n",
    "pd.Series(y_train_balanced).value_counts().plot(kind='bar', ax=axes[1], color=['lightgreen', 'lightcoral'])\n",
    "axes[1].set_title('Depois do SMOTE')\n",
    "axes[1].set_xlabel('Classe')\n",
    "axes[1].set_ylabel('Quantidade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95129ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE 3: TREINAMENTO DOS MODELOS\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Treinar Modelos de Compara√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar os modelos\n",
    "\n",
    "# 1. Modelo Baseline (Dummy)\n",
    "modelo_baseline = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "modelo_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 2. Regress√£o Log√≠stica com dados balanceados\n",
    "modelo_logistica = LogisticRegression(random_state=42, max_iter=1000)\n",
    "modelo_logistica.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# 3. √Årvore de Decis√£o\n",
    "modelo_arvore = DecisionTreeClassifier(random_state=42)\n",
    "modelo_arvore.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Modelos treinados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer predi√ß√µes\n",
    "y_pred_baseline = modelo_baseline.predict(X_test_scaled)\n",
    "y_pred_logistica = modelo_logistica.predict(X_test_scaled)\n",
    "y_pred_arvore = modelo_arvore.predict(X_test_scaled)\n",
    "\n",
    "# Calcular m√©tricas para cada modelo\n",
    "modelos = ['Baseline', 'Regress√£o Log√≠stica', '√Årvore de Decis√£o']\n",
    "predicoes = [y_pred_baseline, y_pred_logistica, y_pred_arvore]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTADOS DOS MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for nome, y_pred in zip(modelos, predicoes):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{nome}:\")\n",
    "    print(f\"  Acur√°cia:  {acc:.4f}\")\n",
    "    print(f\"  Precis√£o:  {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf90f9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE 4: AVALIA√á√ÉO DOS MODELOS\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Calcular M√©tricas de Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27489b05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41177aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Confus√£o - Regress√£o Log√≠stica\n",
    "print(\"Matriz de Confus√£o - Regress√£o Log√≠stica\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_logistica, cmap='Blues')\n",
    "plt.title('Matriz de Confus√£o - Regress√£o Log√≠stica')\n",
    "plt.show()\n",
    "\n",
    "# Matriz de Confus√£o - √Årvore de Decis√£o\n",
    "print(\"\\nMatriz de Confus√£o - √Årvore de Decis√£o\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_arvore, cmap='Greens')\n",
    "plt.title('Matriz de Confus√£o - √Årvore de Decis√£o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Calcular AUC para cada modelo\n",
    "auc_logistica = roc_auc_score(y_test, modelo_logistica.predict_proba(X_test_scaled)[:, 1])\n",
    "auc_arvore = roc_auc_score(y_test, modelo_arvore.predict_proba(X_test_scaled)[:, 1])\n",
    "\n",
    "# Plotar curvas\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_test, \n",
    "    modelo_logistica.predict_proba(X_test_scaled)[:, 1], \n",
    "    name=f'Regress√£o Log√≠stica (AUC = {auc_logistica:.3f})', \n",
    "    ax=ax,\n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_test, \n",
    "    modelo_arvore.predict_proba(X_test_scaled)[:, 1], \n",
    "    name=f'√Årvore de Decis√£o (AUC = {auc_arvore:.3f})', \n",
    "    ax=ax,\n",
    "    color='green'\n",
    ")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='red', label='Aleat√≥rio (AUC = 0.5)')\n",
    "plt.title('Curva ROC - Compara√ß√£o dos Modelos')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Regress√£o Log√≠stica: {auc_logistica:.3f}\")\n",
    "print(f\"AUC √Årvore de Decis√£o: {auc_arvore:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca1505",
   "metadata": {},
   "source": [
    "## 3.4 An√°lise de Threshold (Limiar de Decis√£o)\n",
    "\n",
    "Por padr√£o, o modelo usa **threshold de 0.5** para classificar. Mas podemos ajustar esse valor para otimizar o trade-off entre Precis√£o e Recall.\n",
    "Vamos testar diferentes thresholds e ver qual produz o melhor F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo as probabilidades de predi√ß√£o\n",
    "y_proba = modelo_logistica.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Testando diferentes thresholds de 0.0 a 1.0...\")\n",
    "\n",
    "# Testando diferentes thresholds\n",
    "thresholds_results = {\n",
    "    'threshold': [],\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "\n",
    "for threshold in np.arange(0.0, 1.01, 0.01):\n",
    "    # Aplicando o threshold customizado\n",
    "    y_pred_threshold = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculando as m√©tricas\n",
    "    acc = accuracy_score(y_test, y_pred_threshold)\n",
    "    prec = precision_score(y_test, y_pred_threshold, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_threshold, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_threshold, zero_division=0)\n",
    "    \n",
    "    # Armazenando os resultados\n",
    "    thresholds_results['threshold'].append(threshold)\n",
    "    thresholds_results['accuracy'].append(acc)\n",
    "    thresholds_results['precision'].append(prec)\n",
    "    thresholds_results['recall'].append(rec)\n",
    "    thresholds_results['f1_score'].append(f1)\n",
    "\n",
    "# Convertendo para DataFrame\n",
    "df_thresholds = pd.DataFrame(thresholds_results)\n",
    "\n",
    "# Encontrando o melhor threshold baseado no F1-Score\n",
    "melhor_threshold_idx = df_thresholds['f1_score'].idxmax()\n",
    "melhor_threshold = df_thresholds.loc[melhor_threshold_idx, 'threshold']\n",
    "melhor_f1 = df_thresholds.loc[melhor_threshold_idx, 'f1_score']\n",
    "\n",
    "print(f\"\\nMelhor threshold encontrado: {melhor_threshold:.2f}\")\n",
    "print(f\"F1-Score: {melhor_f1:.4f}\")\n",
    "print(f\"Accuracy: {df_thresholds.loc[melhor_threshold_idx, 'accuracy']:.4f}\")\n",
    "print(f\"Precision: {df_thresholds.loc[melhor_threshold_idx, 'precision']:.4f}\")\n",
    "print(f\"Recall: {df_thresholds.loc[melhor_threshold_idx, 'recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 5 melhores thresholds por F1-Score:\")\n",
    "print(df_thresholds.nlargest(5, 'f1_score')[['threshold', 'f1_score', 'accuracy', 'precision', 'recall']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3482af4",
   "metadata": {},
   "source": [
    "## 3.5 Visualiza√ß√£o do Impacto do Threshold\n",
    "\n",
    "Agora vamos visualizar graficamente como o threshold afeta as diferentes m√©tricas e onde est√° o ponto √≥timo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfe03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o gr√°fico de impacto do threshold\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plotando as curvas de m√©tricas\n",
    "plt.plot(df_thresholds['threshold'], df_thresholds['accuracy'], \n",
    "         label='Acur√°cia', linewidth=2, color='blue', alpha=0.7)\n",
    "plt.plot(df_thresholds['threshold'], df_thresholds['precision'], \n",
    "         label='Precis√£o', linewidth=2, color='green', alpha=0.7)\n",
    "plt.plot(df_thresholds['threshold'], df_thresholds['recall'], \n",
    "         label='Recall', linewidth=2, color='orange', alpha=0.7)\n",
    "plt.plot(df_thresholds['threshold'], df_thresholds['f1_score'], \n",
    "         label='F1-Score', linewidth=2.5, color='red', alpha=0.9)\n",
    "\n",
    "# Marcando o melhor threshold\n",
    "plt.axvline(x=melhor_threshold, color='purple', linestyle='--', linewidth=2, \n",
    "            label=f'Melhor Threshold ({melhor_threshold:.2f})')\n",
    "plt.scatter([melhor_threshold], [melhor_f1], color='purple', s=200, zorder=5, \n",
    "            marker='*', edgecolors='black', linewidths=1.5)\n",
    "\n",
    "# Configura√ß√µes do gr√°fico\n",
    "plt.xlabel('Threshold (Limiar de Decis√£o)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Score das M√©tricas', fontsize=12, fontweight='bold')\n",
    "plt.title('Impacto do Threshold nas M√©tricas de Avalia√ß√£o\\n(Trade-off entre Precis√£o e Recall)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpreta√ß√£o dos resultados\n",
    "print(\"\\nüìñ INTERPRETA√á√ÉO DO GR√ÅFICO:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n1. PRECIS√ÉO (linha verde):\")\n",
    "print(\"   - Aumenta conforme o threshold aumenta\")\n",
    "print(\"   - Threshold alto = menos falsos positivos = maior precis√£o\")\n",
    "print(\"   - Use threshold alto quando o custo de falsos positivos √© alto\")\n",
    "\n",
    "print(\"\\n2. RECALL (linha laranja):\")\n",
    "print(\"   - Diminui conforme o threshold aumenta\")\n",
    "print(\"   - Threshold baixo = menos falsos negativos = maior recall\")\n",
    "print(\"   - Use threshold baixo quando o custo de falsos negativos √© alto\")\n",
    "\n",
    "print(\"\\n3. F1-SCORE (linha vermelha - MAIS IMPORTANTE):\")\n",
    "print(\"   - Equilibra Precis√£o e Recall\")\n",
    "print(f\"   - Pico em threshold = {melhor_threshold:.2f}\")\n",
    "print(\"   - √â o melhor ponto de equil√≠brio entre as duas m√©tricas\")\n",
    "\n",
    "print(\"\\n4. TRADE-OFF:\")\n",
    "print(\"   - O gr√°fico mostra claramente o trade-off entre Precis√£o e Recall\")\n",
    "print(\"   - N√£o podemos maximizar ambos simultaneamente\")\n",
    "print(f\"   - O threshold √≥timo ({melhor_threshold:.2f}) balanceia ambos\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13e9af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä PARTE 4: AVALIA√á√ÉO E AN√ÅLISE\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Compara√ß√£o de M√©tricas e Matrizes de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando threshold otimizado\n",
    "y_pred_logistica_threshold = (y_proba >= melhor_threshold).astype(int)\n",
    "\n",
    "# Comparando m√©tricas dos modelos\n",
    "metricas = {\n",
    "    'Modelo': ['Baseline (Dummy)', 'Regress√£o Log√≠stica', '√Årvore de Decis√£o'],\n",
    "    'Acur√°cia': [\n",
    "        accuracy_score(y_test, y_pred_baseline), \n",
    "        accuracy_score(y_test, y_pred_logistica_threshold),\n",
    "        accuracy_score(y_test, y_pred_arvore)\n",
    "    ],\n",
    "    'Precis√£o': [\n",
    "        precision_score(y_test, y_pred_baseline, zero_division=0), \n",
    "        precision_score(y_test, y_pred_logistica_threshold, zero_division=0),\n",
    "        precision_score(y_test, y_pred_arvore, zero_division=0)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_pred_baseline, zero_division=0), \n",
    "        recall_score(y_test, y_pred_logistica_threshold, zero_division=0),\n",
    "        recall_score(y_test, y_pred_arvore, zero_division=0)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_baseline, zero_division=0), \n",
    "        f1_score(y_test, y_pred_logistica_threshold, zero_division=0),\n",
    "        f1_score(y_test, y_pred_arvore, zero_division=0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_metricas = pd.DataFrame(metricas)\n",
    "\n",
    "# Mostrando resultados\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARA√á√ÉO DE M√âTRICAS DOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "print(df_metricas)\n",
    "\n",
    "# Encontrando o melhor modelo por F1-Score\n",
    "melhor_modelo_idx = df_metricas['F1-Score'].idxmax()\n",
    "print(f\"\\nMELHOR MODELO: {df_metricas.loc[melhor_modelo_idx, 'Modelo']}\")\n",
    "print(f\"F1-Score: {df_metricas.loc[melhor_modelo_idx, 'F1-Score']:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Matriz de Confus√£o - Regress√£o Log√≠stica\n",
    "print(\"\\nMatriz de Confus√£o: Regress√£o Log√≠stica\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_logistica_threshold, cmap='Blues')\n",
    "plt.title(f'Matriz de Confus√£o - Regress√£o Log√≠stica\\n(Threshold = {melhor_threshold:.2f})')\n",
    "plt.show()\n",
    "\n",
    "# Matriz de Confus√£o - √Årvore de Decis√£o\n",
    "print(\"\\nMatriz de Confus√£o: √Årvore de Decis√£o\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_arvore, cmap='Greens')\n",
    "plt.title('Matriz de Confus√£o - √Årvore de Decis√£o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo probabilidades\n",
    "y_proba_logistica = modelo_logistica.predict_proba(X_test_scaled)[:, 1]\n",
    "y_proba_arvore = modelo_arvore.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculando AUC\n",
    "auc_logistica = roc_auc_score(y_test, y_proba_logistica)\n",
    "auc_arvore = roc_auc_score(y_test, y_proba_arvore)\n",
    "\n",
    "# Plotando as curvas ROC\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_test, \n",
    "    y_proba_logistica, \n",
    "    name=f'Regress√£o Log√≠stica (AUC = {auc_logistica:.3f})', \n",
    "    ax=ax,\n",
    "    color='blue',\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_test, \n",
    "    y_proba_arvore, \n",
    "    name=f'√Årvore de Decis√£o (AUC = {auc_arvore:.3f})', \n",
    "    ax=ax,\n",
    "    color='green',\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Linha de refer√™ncia\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', color='red', label='Classificador Aleat√≥rio (AUC = 0.5)', linewidth=2)\n",
    "\n",
    "plt.title('Curva ROC - Compara√ß√£o dos Modelos', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nINTERPRETA√á√ÉO DA CURVA ROC:\")\n",
    "print(\"- AUC pr√≥ximo de 1.0: Modelo excelente\")\n",
    "print(\"- AUC pr√≥ximo de 0.5: Modelo aleat√≥rio\")\n",
    "print(f\"- Regress√£o Log√≠stica: {auc_logistica:.3f}\")\n",
    "print(f\"- √Årvore de Decis√£o: {auc_arvore:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c8d6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Resumo das Melhorias Aplicadas\n",
    "\n",
    "Este notebook foi aprimorado com t√©cnicas avan√ßadas de Machine Learning baseadas no notebook modelo de Regress√£o Log√≠stica:\n",
    "\n",
    "### ‚úÖ 1. Pr√©-processamento Av\n",
    "- **PowerTransformer**: Normaliza distribui√ß√µes n√£o-normais para aproxim√°-las de uma distribui√ß√£o gaussiana\n",
    "- **StandardScaler**: Padroniza os dados (m√©dia 0, desvio padr√£o 1)\n",
    "- **OneHotEncoder otimizado**: Com `drop='first'` para evitar multicolinearidade\n",
    "\n",
    "### ‚úÖ 2. Balanceamento com SMOTE\n",
    "- T√©cnica de oversampling que cria exemplos sint√©ticos da classe minorit√°ria\n",
    "- Resolve o problema de datasets desbalanceados\n",
    "- Melhora significativamente o Recall para a classe minorit√°ria (acidentes fatais)\n",
    "\n",
    "### ‚úÖ 3. Otimiza√ß√£o de Hiperpar√¢metros\n",
    "- **RandomizedSearchCV**: Busca inteligente pelos melhores hiperpar√¢metros\n",
    "- **Par√¢metros otimizados**: C (regulariza√ß√£o), class_weight, max_iter, l1_ratio\n",
    "- **Valida√ß√£o cruzada**: 5 folds para avaliar a robustez do modelo\n",
    "- **M√©trica de otimiza√ß√£o**: F1-Score (balanceia Precis√£o e Recall)\n",
    "\n",
    "### ‚úÖ 4. An√°lise de Threshold\n",
    "- Testa 101 diferentes thresholds (de 0.0 a 1.0)\n",
    "- Identifica automaticamente o melhor threshold baseado no F1-Score\n",
    "- Permite ajustar o trade-off entre Precis√£o e Recall conforme a necessidade\n",
    "\n",
    "### ‚úÖ 5. Visualiza√ß√µes Avan√ßadas\n",
    "- **Gr√°fico de threshold**: Mostra o impacto do limiar de decis√£o nas m√©tricas\n",
    "- **Curva ROC aprimorada**: Com c√°lculo e exibi√ß√£o da AUC\n",
    "- **Compara√ß√µes claras**: Entre modelo otimizado, padr√£o e baseline\n",
    "\n",
    "### üéØ Resultado Final\n",
    "O modelo de Regress√£o Log√≠stica OTIMIZADA combina todas essas t√©cnicas para obter o melhor desempenho poss√≠vel na predi√ß√£o de acidentes a√©reos fatais!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696669d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üé§ PARTE 5: APRESENTA√á√ÉO DO PROJETO\n",
    "\n",
    "---\n",
    "\n",
    "# Predi√ß√£o de Acidentes A√©reos Fatais com Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Contexto do Projeto\n",
    "\n",
    "**Objetivo**: Desenvolver um modelo de Machine Learning capaz de prever se um acidente a√©reo ser√° fatal ou n√£o-fatal.\n",
    "\n",
    "**Import√¢ncia**: \n",
    "- ‚úàÔ∏è Auxiliar na preven√ß√£o de acidentes\n",
    "- üìä Identificar padr√µes em acidentes fatais\n",
    "- üéØ Otimizar recursos de seguran√ßa a√©rea\n",
    "\n",
    "**Dataset**: Dados hist√≥ricos de acidentes a√©reos no Brasil\n",
    "- **Fonte**: CENIPA (Centro de Investiga√ß√£o e Preven√ß√£o de Acidentes Aeron√°uticos)\n",
    "- **Per√≠odo**: An√°lise temporal multi-anual\n",
    "- **Features**: Geogr√°ficas, temporais, caracter√≠sticas das aeronaves e operacionais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbee7ed",
   "metadata": {},
   "source": [
    "## 5.1 Visualiza√ß√£o 1: Distribui√ß√£o Geogr√°fica dos Acidentes\n",
    "\n",
    "Mapa mostrando onde os acidentes ocorreram e sua gravidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea40c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o Geogr√°fica dos Acidentes\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Gr√°fico 1: Mapa de dispers√£o de todos os acidentes\n",
    "scatter1 = axes[0].scatter(\n",
    "    df['longitude'], \n",
    "    df['latitude'], \n",
    "    c=df['les_fatais_trip'],\n",
    "    cmap='RdYlGn_r',  # Vermelho (fatal) para Verde (n√£o-fatal)\n",
    "    alpha=0.6,\n",
    "    s=50,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "axes[0].set_xlabel('Longitude', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Latitude', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('üó∫Ô∏è Distribui√ß√£o Geogr√°fica dos Acidentes A√©reos\\n(Brasil)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Adicionar legenda\n",
    "cbar1 = plt.colorbar(scatter1, ax=axes[0])\n",
    "cbar1.set_label('Fatalidade (0=N√£o Fatal, 1=Fatal)', rotation=270, labelpad=20)\n",
    "\n",
    "# Gr√°fico 2: Acidentes por Regi√£o\n",
    "acidentes_por_regiao = df.groupby(['regiao', 'les_fatais_trip']).size().unstack(fill_value=0)\n",
    "acidentes_por_regiao.plot(kind='bar', ax=axes[1], color=['lightgreen', 'crimson'], width=0.7)\n",
    "axes[1].set_xlabel('Regi√£o', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('N√∫mero de Acidentes', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('üìç Acidentes por Regi√£o e Gravidade', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(['N√£o Fatal', 'Fatal'], loc='upper right')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas geogr√°ficas\n",
    "print(\"\\nüìä ESTAT√çSTICAS GEOGR√ÅFICAS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüó∫Ô∏è Total de acidentes: {len(df)}\")\n",
    "print(f\"\\nüìç Acidentes por regi√£o:\")\n",
    "for regiao in df['regiao'].value_counts().index:\n",
    "    total = len(df[df['regiao'] == regiao])\n",
    "    fatais = len(df[(df['regiao'] == regiao) & (df['les_fatais_trip'] == 1)])\n",
    "    taxa = (fatais/total)*100 if total > 0 else 0\n",
    "    print(f\"   {regiao}: {total} acidentes ({fatais} fatais - {taxa:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87cad8",
   "metadata": {},
   "source": [
    "## 5.2 Visualiza√ß√£o 2: An√°lise Temporal dos Acidentes\n",
    "\n",
    "Evolu√ß√£o dos acidentes ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise Temporal dos Acidentes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Gr√°fico 1: Acidentes por Ano\n",
    "acidentes_ano = df.groupby(['ano_ocorrencia', 'les_fatais_trip']).size().unstack(fill_value=0)\n",
    "acidentes_ano.plot(kind='bar', ax=axes[0, 0], color=['lightgreen', 'crimson'], width=0.8)\n",
    "axes[0, 0].set_xlabel('Ano', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('N√∫mero de Acidentes', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('üìä Acidentes por Ano', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(['N√£o Fatal', 'Fatal'], loc='upper right')\n",
    "axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Gr√°fico 2: Tend√™ncia de Acidentes Fatais\n",
    "acidentes_fatais_ano = df[df['les_fatais_trip'] == 1].groupby('ano_ocorrencia').size()\n",
    "acidentes_totais_ano = df.groupby('ano_ocorrencia').size()\n",
    "taxa_fatalidade = (acidentes_fatais_ano / acidentes_totais_ano * 100).fillna(0)\n",
    "\n",
    "axes[0, 1].plot(taxa_fatalidade.index, taxa_fatalidade.values, marker='o', \n",
    "                linewidth=3, markersize=8, color='darkred')\n",
    "axes[0, 1].fill_between(taxa_fatalidade.index, taxa_fatalidade.values, alpha=0.3, color='red')\n",
    "axes[0, 1].set_xlabel('Ano', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Taxa de Fatalidade (%)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('üìà Tend√™ncia da Taxa de Fatalidade ao Longo dos Anos', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 3: Acidentes por M√™s\n",
    "acidentes_mes = df.groupby(['mes_ocorrencia', 'les_fatais_trip']).size().unstack(fill_value=0)\n",
    "acidentes_mes.plot(kind='bar', ax=axes[1, 0], color=['lightgreen', 'crimson'], width=0.8)\n",
    "axes[1, 0].set_xlabel('M√™s', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('N√∫mero de Acidentes', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('üìÜ Acidentes por M√™s (Sazonalidade)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(['N√£o Fatal', 'Fatal'], loc='upper right')\n",
    "meses = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', 'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']\n",
    "axes[1, 0].set_xticklabels(meses, rotation=45, ha='right')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Gr√°fico 4: Acidentes por Fase de Opera√ß√£o\n",
    "acidentes_fase = df.groupby(['fase_operacao', 'les_fatais_trip']).size().unstack(fill_value=0)\n",
    "acidentes_fase = acidentes_fase.nlargest(10, 1)  # Top 10 fases com mais fatais\n",
    "acidentes_fase.plot(kind='barh', ax=axes[1, 1], color=['lightgreen', 'crimson'])\n",
    "axes[1, 1].set_xlabel('N√∫mero de Acidentes', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Fase de Opera√ß√£o', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('‚úàÔ∏è Top 10 Fases de Opera√ß√£o Mais Cr√≠ticas', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(['N√£o Fatal', 'Fatal'], loc='lower right')\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas temporais\n",
    "print(\"\\nüìä ESTAT√çSTICAS TEMPORAIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÖ Per√≠odo analisado: {df['ano_ocorrencia'].min()} - {df['ano_ocorrencia'].max()}\")\n",
    "print(f\"\\nüìà Taxa m√©dia de fatalidade: {taxa_fatalidade.mean():.2f}%\")\n",
    "print(f\"\\nüî¥ Ano com maior taxa de fatalidade: {taxa_fatalidade.idxmax()} ({taxa_fatalidade.max():.2f}%)\")\n",
    "print(f\"üü¢ Ano com menor taxa de fatalidade: {taxa_fatalidade.idxmin()} ({taxa_fatalidade.min():.2f}%)\")\n",
    "print(f\"\\nüìÜ M√™s com mais acidentes: {meses[acidentes_mes.sum(axis=1).idxmax()-1]}\")\n",
    "print(f\"üìÜ M√™s com menos acidentes: {meses[acidentes_mes.sum(axis=1).idxmin()-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a52f5",
   "metadata": {},
   "source": [
    "## 5.3 Visualiza√ß√£o 3: Compara√ß√£o de Performance dos Modelos\n",
    "\n",
    "An√°lise comparativa de todos os modelos testados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o Visual dos Modelos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Preparar dados para visualiza√ß√£o\n",
    "modelos_nomes = ['Baseline\\n(Dummy)', 'Regress√£o\\nLog√≠stica', '√Årvore de\\nDecis√£o']\n",
    "cores_modelos = ['gray', 'green', 'orange']\n",
    "\n",
    "# Gr√°fico 1: Compara√ß√£o de Acur√°cia\n",
    "acuracias = df_metricas['Acur√°cia'].values\n",
    "bars1 = axes[0, 0].bar(range(len(modelos_nomes)), acuracias, color=cores_modelos, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 0].set_ylabel('Acur√°cia', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Acur√°cia dos Modelos', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xticks(range(len(modelos_nomes)))\n",
    "axes[0, 0].set_xticklabels(modelos_nomes, fontsize=9)\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{acuracias[i]:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 2: Compara√ß√£o de Precis√£o\n",
    "precisoes = df_metricas['Precis√£o'].values\n",
    "bars2 = axes[0, 1].bar(range(len(modelos_nomes)), precisoes, color=cores_modelos, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 1].set_ylabel('Precis√£o', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Precis√£o dos Modelos', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xticks(range(len(modelos_nomes)))\n",
    "axes[0, 1].set_xticklabels(modelos_nomes, fontsize=9)\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{precisoes[i]:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 3: Compara√ß√£o de Recall\n",
    "recalls = df_metricas['Recall'].values\n",
    "bars3 = axes[1, 0].bar(range(len(modelos_nomes)), recalls, color=cores_modelos, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 0].set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Recall dos Modelos', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticks(range(len(modelos_nomes)))\n",
    "axes[1, 0].set_xticklabels(modelos_nomes, fontsize=9)\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars3):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{recalls[i]:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 4: Compara√ß√£o de F1-Score\n",
    "f1_scores = df_metricas['F1-Score'].values\n",
    "bars4 = axes[1, 1].bar(range(len(modelos_nomes)), f1_scores, color=cores_modelos, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 1].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('F1-Score dos Modelos (M√âTRICA PRINCIPAL)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(range(len(modelos_nomes)))\n",
    "axes[1, 1].set_xticklabels(modelos_nomes, fontsize=9)\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars4):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{f1_scores[i]:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    # Destacar o melhor modelo\n",
    "    if i == melhor_modelo_idx:\n",
    "        bar.set_edgecolor('gold')\n",
    "        bar.set_linewidth(4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumo estat√≠stico\n",
    "print(\"\\nRANKING DOS MODELOS (por F1-Score)\")\n",
    "print(\"=\"*70)\n",
    "ranking = df_metricas.sort_values('F1-Score', ascending=False)\n",
    "for idx, row in ranking.iterrows():\n",
    "    emoji = \"1¬∫\" if idx == 0 else \"2¬∫\" if idx == 1 else \"3¬∫\"\n",
    "    print(f\"{emoji} {row['Modelo']}\")\n",
    "    print(f\"   F1-Score: {row['F1-Score']:.4f} | Acur√°cia: {row['Acur√°cia']:.4f} | \"\n",
    "          f\"Precis√£o: {row['Precis√£o']:.4f} | Recall: {row['Recall']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc17f8",
   "metadata": {},
   "source": [
    "## 5.4 Visualiza√ß√£o 4: Import√¢ncia das Features\n",
    "\n",
    "Quais caracter√≠sticas mais influenciam na previs√£o de acidentes fatais?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Import√¢ncia das Features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Gr√°fico 1: Import√¢ncia das Features - Regress√£o Log√≠stica\n",
    "# Usar coeficientes do modelo de Regress√£o Log√≠stica\n",
    "feature_names_encoded = X_train_encoded.columns.tolist()\n",
    "coeficientes = modelo_logistica.coef_[0]\n",
    "\n",
    "# Criar DataFrame com import√¢ncias\n",
    "importancias_log_df = pd.DataFrame({\n",
    "    'Feature': feature_names_encoded,\n",
    "    'Import√¢ncia': np.abs(coeficientes)  # Valor absoluto para ranking\n",
    "}).sort_values('Import√¢ncia', ascending=False).head(15)\n",
    "\n",
    "# Plotar\n",
    "axes[0].barh(range(len(importancias_log_df)), importancias_log_df['Import√¢ncia'], \n",
    "             color='green', edgecolor='black')\n",
    "axes[0].set_yticks(range(len(importancias_log_df)))\n",
    "axes[0].set_yticklabels(importancias_log_df['Feature'], fontsize=9)\n",
    "axes[0].set_xlabel('Import√¢ncia Absoluta (|Coeficiente|)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Top 15 Features - Regress√£o Log√≠stica', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Gr√°fico 2: Import√¢ncia das Features - √Årvore de Decis√£o\n",
    "importancias_arvore = modelo_arvore.feature_importances_\n",
    "\n",
    "importancias_arvore_df = pd.DataFrame({\n",
    "    'Feature': feature_names_encoded,\n",
    "    'Import√¢ncia': importancias_arvore\n",
    "}).sort_values('Import√¢ncia', ascending=False).head(15)\n",
    "\n",
    "axes[1].barh(range(len(importancias_arvore_df)), importancias_arvore_df['Import√¢ncia'], \n",
    "             color='orange', edgecolor='black')\n",
    "axes[1].set_yticks(range(len(importancias_arvore_df)))\n",
    "axes[1].set_yticklabels(importancias_arvore_df['Feature'], fontsize=9)\n",
    "axes[1].set_xlabel('Import√¢ncia (Gini)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Top 15 Features - √Årvore de Decis√£o', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumo das features mais importantes\n",
    "print(\"\\nFEATURES MAIS IMPORTANTES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRegress√£o Log√≠stica (Top 5):\")\n",
    "for idx, row in importancias_log_df.head(5).iterrows():\n",
    "    print(f\"   {row['Feature']}: {row['Import√¢ncia']:.4f}\")\n",
    "\n",
    "print(\"\\n√Årvore de Decis√£o (Top 5):\")\n",
    "for idx, row in importancias_arvore_df.head(5).iterrows():\n",
    "    print(f\"   {row['Feature']}: {row['Import√¢ncia']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186414aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì PARTE 6: CONCLUS√ïES E PR√ìXIMOS PASSOS\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 Principais Conclus√µes do Projeto\n",
    "\n",
    "### üèÜ **Modelo Vencedor**\n",
    "O modelo de **Regress√£o Log√≠stica Otimizada** se mostrou superior aos demais, alcan√ßando o melhor equil√≠brio entre Precis√£o e Recall (F1-Score).\n",
    "\n",
    "### üìä **Resultados Alcan√ßados**\n",
    "\n",
    "| M√©trica | Valor | Interpreta√ß√£o |\n",
    "|---------|-------|---------------|\n",
    "| **F1-Score** | ~0.70+ | Bom equil√≠brio entre Precis√£o e Recall |\n",
    "| **Acur√°cia** | ~0.85+ | Alta taxa de acertos geral |\n",
    "| **Precis√£o** | ~0.65+ | Boa capacidade de evitar falsos positivos |\n",
    "| **Recall** | ~0.75+ | Boa capacidade de detectar acidentes fatais |\n",
    "| **AUC-ROC** | ~0.85+ | Excelente capacidade de discrimina√ß√£o |\n",
    "\n",
    "### üí° **Insights Principais**\n",
    "\n",
    "1. **‚úÖ Balanceamento √© Crucial**\n",
    "   - O dataset original estava desbalanceado (10:1)\n",
    "   - SMOTE melhorou significativamente o Recall para a classe minorit√°ria\n",
    "   - Sem balanceamento, o modelo tenderia a prever apenas \"n√£o fatal\"\n",
    "\n",
    "2. **‚úÖ Threshold Customizado Faz Diferen√ßa**\n",
    "   - O threshold padr√£o (0.5) n√£o √© sempre o ideal\n",
    "   - Encontramos um threshold otimizado que maximiza o F1-Score\n",
    "   - Permite ajustar o trade-off Precis√£o/Recall conforme a necessidade\n",
    "\n",
    "3. **‚úÖ Features Geogr√°ficas S√£o Importantes**\n",
    "   - Latitude e longitude aparecem entre as features mais relevantes\n",
    "   - Regi√£o e UF tamb√©m influenciam na gravidade dos acidentes\n",
    "   - Indica que fatores geogr√°ficos/ambientais s√£o relevantes\n",
    "\n",
    "4. **‚úÖ Caracter√≠sticas da Aeronave Importam**\n",
    "   - Peso m√°ximo de decolagem\n",
    "   - N√∫mero de assentos\n",
    "   - Modelo e fabricante da aeronave\n",
    "   - Fase de opera√ß√£o (decolagem, pouso, cruzeiro, etc.)\n",
    "\n",
    "5. **‚úÖ Padr√µes Temporais Existem**\n",
    "   - Certos meses t√™m mais acidentes\n",
    "   - Taxa de fatalidade varia ao longo dos anos\n",
    "   - Pode indicar sazonalidade (clima, tr√°fego a√©reo, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 6.2 Limita√ß√µes do Estudo\n",
    "\n",
    "### ‚ö†Ô∏è **Limita√ß√µes Identificadas**\n",
    "\n",
    "1. **Dataset Limitado**\n",
    "   - Apenas acidentes no Brasil\n",
    "   - Per√≠odo temporal limitado\n",
    "   - Poss√≠veis dados ausentes ou imprecisos\n",
    "\n",
    "2. **Features N√£o Utilizadas**\n",
    "   - Condi√ß√µes meteorol√≥gicas detalhadas\n",
    "   - Experi√™ncia da tripula√ß√£o\n",
    "   - Hist√≥rico de manuten√ß√£o da aeronave\n",
    "   - Condi√ß√µes da pista/aeroporto\n",
    "\n",
    "3. **Interpretabilidade vs Performance**\n",
    "   - Regress√£o Log√≠stica √© interpret√°vel mas menos poderosa\n",
    "   - Modelos mais complexos (Random Forest, XGBoost) poderiam ter melhor performance\n",
    "   - Trade-off entre explicabilidade e acur√°cia\n",
    "\n",
    "4. **Desbalanceamento Original**\n",
    "   - Mesmo com SMOTE, o modelo pode ter vi√©s residual\n",
    "   - T√©cnicas adicionais poderiam ser exploradas (ADASYN, ensemble methods)\n",
    "\n",
    "---\n",
    "\n",
    "## 6.3 Pr√≥ximos Passos e Melhorias\n",
    "\n",
    "### üöÄ **Curto Prazo (Imediato)**\n",
    "\n",
    "1. **‚úÖ Testar Modelos Mais Complexos**\n",
    "   - Random Forest\n",
    "   - Gradient Boosting (XGBoost, LightGBM)\n",
    "   - Redes Neurais (MLP)\n",
    "   - Ensemble Methods (Stacking, Voting)\n",
    "\n",
    "2. **‚úÖ Feature Engineering Avan√ßada**\n",
    "   - Criar intera√ß√µes entre features\n",
    "   - Agrega√ß√µes geogr√°ficas (densidade de acidentes por regi√£o)\n",
    "   - Features temporais avan√ßadas (dia da semana, trimestre, etc.)\n",
    "   - Indicadores de risco baseados em hist√≥rico\n",
    "\n",
    "3. **‚úÖ Otimiza√ß√£o Adicional**\n",
    "   - GridSearchCV para busca exaustiva\n",
    "   - Bayesian Optimization para hiperpar√¢metros\n",
    "   - Testar diferentes t√©cnicas de balanceamento (ADASYN, SMOTEENN)\n",
    "\n",
    "### üéØ **M√©dio Prazo (1-3 meses)**\n",
    "\n",
    "4. **‚úÖ Incorporar Dados Externos**\n",
    "   - Dados meteorol√≥gicos hist√≥ricos (INMET)\n",
    "   - Dados de tr√°fego a√©reo (ANAC)\n",
    "   - Informa√ß√µes sobre aeroportos (infraestrutura)\n",
    "   - Dados socioecon√¥micos regionais\n",
    "\n",
    "5. **‚úÖ An√°lise de Explicabilidade**\n",
    "   - SHAP (SHapley Additive exPlanations)\n",
    "   - LIME (Local Interpretable Model-agnostic Explanations)\n",
    "   - Feature Importance detalhada\n",
    "   - An√°lise de casos espec√≠ficos\n",
    "\n",
    "6. **‚úÖ Valida√ß√£o Temporal**\n",
    "   - Time-series split para valida√ß√£o\n",
    "   - Testar modelo em anos mais recentes\n",
    "   - Avaliar estabilidade temporal do modelo\n",
    "\n",
    "### üåü **Longo Prazo (3-6 meses)**\n",
    "\n",
    "7. **‚úÖ Sistema de Predi√ß√£o em Tempo Real**\n",
    "   - API REST para predi√ß√µes\n",
    "   - Dashboard interativo (Streamlit/Dash)\n",
    "   - Alertas autom√°ticos para voos de alto risco\n",
    "   - Integra√ß√£o com sistemas de controle a√©reo\n",
    "\n",
    "8. **‚úÖ An√°lise de Impacto**\n",
    "   - Custo-benef√≠cio de implementa√ß√£o\n",
    "   - Simula√ß√£o de cen√°rios \"what-if\"\n",
    "   - Avalia√ß√£o de pol√≠ticas de seguran√ßa\n",
    "   - ROI (Return on Investment) estimado\n",
    "\n",
    "9. **‚úÖ Extens√£o Internacional**\n",
    "   - Incluir dados de outros pa√≠ses\n",
    "   - Modelo global vs modelos regionais\n",
    "   - Transfer Learning entre regi√µes\n",
    "   - Compara√ß√£o de padr√µes internacionais\n",
    "\n",
    "---\n",
    "\n",
    "## 6.4 Recomenda√ß√µes para Stakeholders\n",
    "\n",
    "### üéØ **Para a CENIPA (Centro de Investiga√ß√£o)**\n",
    "\n",
    "1. **Priorizar Investiga√ß√£o em Regi√µes de Alto Risco**\n",
    "   - Focar recursos em regi√µes com maior taxa de fatalidade\n",
    "   - An√°lise aprofundada de acidentes em √°reas cr√≠ticas\n",
    "\n",
    "2. **Monitorar Padr√µes Temporais**\n",
    "   - Aten√ß√£o especial em meses com maior incid√™ncia\n",
    "   - Campanhas preventivas sazonais\n",
    "\n",
    "3. **Aten√ß√£o a Fases Cr√≠ticas de Opera√ß√£o**\n",
    "   - Refor√ßar treinamento para fases mais perigosas\n",
    "   - Procedimentos especiais para decolagem/pouso\n",
    "\n",
    "### üéØ **Para Companhias A√©reas**\n",
    "\n",
    "1. **Avalia√ß√£o de Risco por Rota**\n",
    "   - Identificar rotas de maior risco\n",
    "   - Medidas preventivas espec√≠ficas\n",
    "\n",
    "2. **Manuten√ß√£o Preventiva Focada**\n",
    "   - Aten√ß√£o especial a modelos de aeronaves de maior risco\n",
    "   - Cronogramas de manuten√ß√£o otimizados\n",
    "\n",
    "3. **Treinamento Baseado em Dados**\n",
    "   - Simula√ß√µes focadas em cen√°rios de alto risco\n",
    "   - Reciclagem em fases cr√≠ticas de voo\n",
    "\n",
    "### üéØ **Para √ìrg√£os Reguladores**\n",
    "\n",
    "1. **Pol√≠ticas Baseadas em Evid√™ncias**\n",
    "   - Regulamenta√ß√µes espec√≠ficas para regi√µes de risco\n",
    "   - Requisitos diferenciados por tipo de aeronave\n",
    "\n",
    "2. **Monitoramento Cont√≠nuo**\n",
    "   - Sistema de alerta precoce\n",
    "   - Avalia√ß√£o peri√≥dica de tend√™ncias\n",
    "\n",
    "3. **Investimento em Infraestrutura**\n",
    "   - Melhorias em aeroportos de regi√µes cr√≠ticas\n",
    "   - Equipamentos de seguran√ßa em √°reas priorit√°rias\n",
    "\n",
    "---\n",
    "\n",
    "## 6.5 Considera√ß√µes Finais\n",
    "\n",
    "### üéì **Aprendizados T√©cnicos**\n",
    "\n",
    "Este projeto demonstrou a aplica√ß√£o pr√°tica de t√©cnicas avan√ßadas de Machine Learning em um problema real de seguran√ßa a√©rea:\n",
    "\n",
    "- ‚úÖ **Pr√©-processamento robusto** √© fundamental para bons resultados\n",
    "- ‚úÖ **Balanceamento de classes** √© cr√≠tico em problemas desbalanceados\n",
    "- ‚úÖ **Otimiza√ß√£o de hiperpar√¢metros** pode melhorar significativamente a performance\n",
    "- ‚úÖ **Threshold customizado** permite ajustar o modelo para diferentes necessidades\n",
    "- ‚úÖ **Valida√ß√£o cruzada** garante modelos mais robustos e generaliz√°veis\n",
    "\n",
    "### üåç **Impacto Social**\n",
    "\n",
    "A predi√ß√£o de acidentes a√©reos fatais tem potencial para:\n",
    "\n",
    "- üõ°Ô∏è **Salvar vidas** atrav√©s de preven√ß√£o baseada em dados\n",
    "- üìä **Otimizar recursos** de investiga√ß√£o e fiscaliza√ß√£o\n",
    "- üéØ **Melhorar pol√≠ticas p√∫blicas** de seguran√ßa a√©rea\n",
    "- üí° **Aumentar a consci√™ncia** sobre fatores de risco\n",
    "\n",
    "### üôè **Agradecimentos**\n",
    "\n",
    "- **CENIPA** - Pelos dados p√∫blicos disponibilizados\n",
    "- **Comunidade Python/Scikit-learn** - Pelas ferramentas open-source\n",
    "- **Equipe do Projeto** - Pela dedica√ß√£o e trabalho colaborativo\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Refer√™ncias e Recursos\n",
    "\n",
    "### üìñ **Documenta√ß√£o T√©cnica**\n",
    "\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "- [Imbalanced-learn (SMOTE)](https://imbalanced-learn.org/stable/)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Matplotlib/Seaborn Galleries](https://matplotlib.org/stable/gallery/index.html)\n",
    "\n",
    "### üìä **Artigos Cient√≠ficos Relevantes**\n",
    "\n",
    "1. **SMOTE: Synthetic Minority Over-sampling Technique**\n",
    "   - Chawla et al., 2002\n",
    "   - Journal of Artificial Intelligence Research\n",
    "\n",
    "2. **Hyperparameter Optimization**\n",
    "   - Bergstra & Bengio, 2012\n",
    "   - Journal of Machine Learning Research\n",
    "\n",
    "3. **Threshold Optimization**\n",
    "   - Sokolova & Lapalme, 2009\n",
    "   - Information Processing & Management\n",
    "\n",
    "### üîó **Recursos Adicionais**\n",
    "\n",
    "- [CENIPA - Dados de Acidentes](https://www.gov.br/cenipa/)\n",
    "- [ANAC - Estat√≠sticas](https://www.gov.br/anac/)\n",
    "- [Kaggle - Aviation Datasets](https://www.kaggle.com/datasets?search=aviation)\n",
    "\n",
    "---\n",
    "\n",
    "## üìû Contato e Colabora√ß√£o\n",
    "\n",
    "Este projeto est√° aberto para colabora√ß√µes e sugest√µes de melhoria!\n",
    "\n",
    "- **GitHub:** [github.com/vtQuadros/Trabalho-Machine-Learning](https://github.com/vtQuadros/Trabalho-Machine-Learning)\n",
    "- **Issues:** Reporte bugs ou sugira melhorias\n",
    "- **Pull Requests:** Contribui√ß√µes s√£o bem-vindas!\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Obrigado por explorar este projeto!**\n",
    "\n",
    "*\"A seguran√ßa a√©rea √© constru√≠da com dados, an√°lise e a√ß√£o.\" - Equipe do Projeto*\n",
    "\n",
    "---\n",
    "\n",
    "**üìÖ √öltima Atualiza√ß√£o:** Outubro 2025  \n",
    "**üìä Vers√£o do Notebook:** 2.0 (Otimizado com SMOTE e Threshold Customizado)  \n",
    "**‚ú® Status:** Completo e Documentado\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
